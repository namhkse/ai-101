{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca559b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--TheBloke--Llama-2-7B-Chat-GGUF\\snapshots\\191239b3e26b2882fb562ffccdd1cf0f65402adb\\.\\llama-2-7b-chat.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q2_K - Medium\n",
      "print_info: file size   = 2.63 GiB (3.35 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: printing all EOG tokens:\n",
      "load:   - 2 ('</s>')\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 4096\n",
      "print_info: n_embd_v_gqa     = 4096\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 6.74 B\n",
      "print_info: general.name     = LLaMA v2\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q2_K) (and 226 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_REPACK model buffer size =   336.00 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  2694.32 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.1.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.1.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.2.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.2.attn_k.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.3.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.3.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.4.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.4.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.5.attn_q.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.5.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.6.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.7.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.9.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.10.attn_q.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.10.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.11.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.12.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.12.attn_k.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.13.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.13.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.14.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.15.attn_q.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.15.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.16.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.17.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.19.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.20.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.20.attn_k.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.22.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.23.attn_q.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.23.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.24.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.25.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.25.attn_k.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.26.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.27.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.27.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.28.attn_q.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.28.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.29.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.29.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.30.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.30.attn_k.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.31.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.31.attn_k.weight with q2_K_8x8\n",
      ".....................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 512\n",
      "llama_context: n_ctx_per_seq = 512\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (512) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 512 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   256.00 MiB\n",
      "llama_kv_cache_unified: size =  256.00 MiB (   512 cells,  32 layers,  1/1 seqs), K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 2328\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =    97.51 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '10', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n",
      "llama_perf_context_print:        load time =     457.87 ms\n",
      "llama_perf_context_print: prompt eval time =     457.78 ms /     6 tokens (   76.30 ms per token,    13.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3094.59 ms /    49 runs   (   63.15 ms per token,    15.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    3562.94 ms /    55 tokens\n",
      "llama_perf_context_print:    graphs reused =         47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      " a young woman named Maria lived in a small village nestled in the rolling hills of Tuscany. Unterscheidung between the two regions is not always clear-cut, as both share some common characteristics and are often blurred in their cultural identity.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample 1\n",
    "from main import print_generated_text\n",
    "prompt_text = \"Once upon a time,\"\n",
    "\n",
    "print_generated_text(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f1c1200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--TheBloke--Llama-2-7B-Chat-GGUF\\snapshots\\191239b3e26b2882fb562ffccdd1cf0f65402adb\\.\\llama-2-7b-chat.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q2_K - Medium\n",
      "print_info: file size   = 2.63 GiB (3.35 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: printing all EOG tokens:\n",
      "load:   - 2 ('</s>')\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 4096\n",
      "print_info: n_embd_v_gqa     = 4096\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 6.74 B\n",
      "print_info: general.name     = LLaMA v2\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q2_K) (and 226 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_REPACK model buffer size =   336.00 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  2694.32 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.1.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.1.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.2.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.2.attn_k.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.3.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.3.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.4.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.4.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.5.attn_q.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.5.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.6.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.7.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.9.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.10.attn_q.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.10.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.11.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.12.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.12.attn_k.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.13.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.13.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.14.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.15.attn_q.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.15.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.16.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.17.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.19.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.20.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.20.attn_k.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.22.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.23.attn_q.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.23.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.24.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.25.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.25.attn_k.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.26.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.27.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.27.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.28.attn_q.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.28.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.29.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.29.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.30.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.30.attn_k.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.31.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.31.attn_k.weight with q2_K_8x8\n",
      ".....................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 512\n",
      "llama_context: n_ctx_per_seq = 512\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (512) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 512 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   256.00 MiB\n",
      "llama_kv_cache_unified: size =  256.00 MiB (   512 cells,  32 layers,  1/1 seqs), K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 2328\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =    97.51 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '10', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n",
      "llama_perf_context_print:        load time =     659.84 ms\n",
      "llama_perf_context_print: prompt eval time =     659.76 ms /    14 tokens (   47.13 ms per token,    21.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3151.16 ms /    49 runs   (   64.31 ms per token,    15.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    3822.38 ms /    63 tokens\n",
      "llama_perf_context_print:    graphs reused =         47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "nahm\n",
      "\n",
      "Unsupervised learning is a type of machine learning where the algorithm tries to find patterns or relationships in the data without any labeled examples. In other words, the algorithm has no information about the correct output or target variable. Unsuper\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample 2\n",
    "from main import print_generated_text\n",
    "prompt_text = \"Explain the difference between supervised and unsupervised learning.\"\n",
    "\n",
    "print_generated_text(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5838a592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--TheBloke--Llama-2-7B-Chat-GGUF\\snapshots\\191239b3e26b2882fb562ffccdd1cf0f65402adb\\.\\llama-2-7b-chat.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q2_K - Medium\n",
      "print_info: file size   = 2.63 GiB (3.35 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: printing all EOG tokens:\n",
      "load:   - 2 ('</s>')\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 4096\n",
      "print_info: n_embd_v_gqa     = 4096\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 6.74 B\n",
      "print_info: general.name     = LLaMA v2\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q2_K) (and 226 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_REPACK model buffer size =   336.00 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  2694.32 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.1.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.1.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.2.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.2.attn_k.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.3.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.3.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.4.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.4.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.5.attn_q.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.5.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.6.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.7.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.9.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.10.attn_q.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.10.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.11.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.12.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.12.attn_k.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.13.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.13.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.14.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.15.attn_q.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.15.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.16.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.17.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.19.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.20.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.20.attn_k.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.22.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.23.attn_q.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.23.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.24.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.25.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.25.attn_k.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.26.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.27.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.27.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.28.attn_q.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.28.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.29.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.29.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.30.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.30.attn_k.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.31.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.31.attn_k.weight with q2_K_8x8\n",
      ".....................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 512\n",
      "llama_context: n_ctx_per_seq = 512\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (512) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 512 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   256.00 MiB\n",
      "llama_kv_cache_unified: size =  256.00 MiB (   512 cells,  32 layers,  1/1 seqs), K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 2328\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =    97.51 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '10', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n",
      "llama_perf_context_print:        load time =     543.53 ms\n",
      "llama_perf_context_print: prompt eval time =     543.42 ms /     8 tokens (   67.93 ms per token,    14.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3221.74 ms /    49 runs   (   65.75 ms per token,    15.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    3776.06 ms /    57 tokens\n",
      "llama_perf_context_print:    graphs reused =         47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      " can be used to describe and predict the motion of objects under various conditions. Unterscheidung zwischen “Law” und “Laws” 1.0 out of 5.0 (1) 10. The Newton's laws of motion\n"
     ]
    }
   ],
   "source": [
    "# Sample 3\n",
    "from main import print_generated_text\n",
    "prompt_text = \"Newton’s three laws of motion\"\n",
    "\n",
    "print_generated_text(prompt_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
